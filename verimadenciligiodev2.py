# -*- coding: utf-8 -*-
"""VeriMadenciligiOdev2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jg-9gXw9ahtRx9zLtpbE1GesYgyPXIBV
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.mixture import GaussianMixture
from sklearn.cluster import Birch
# %matplotlib inline

#The dataset that has been used in model training task consists of totally 640
#samples which represent performance measurement from simulation of 2D
#Multiprocessor Optical Interconnection Network.
#Attribute Information:
# Node Number - The Number of Nodes in Network (64 or 16)
# Thread Number - The number of threads in each node at beginning of simulation
# Spatial Distribution - This column indicates performance of the network, UN (Uniform),HR (Hot Region),BR (Bit Reverse),PS (Perfect Shuffle)
# Temporal Distribution - 

df = pd.read_csv('optical_interconnection_network .csv')

df2 = df.copy()

#luckily there is no any missing value in dataset
for z in df2.columns:
  df2 = df2[df2[z].notna()]

if len(df) - len(df2) == 0:
  print('there is no any missing value in dataset')

df.head()

#Column names in dataset
df.columns

#before normalizing part we have to convert some categorical valued features into 
#numeric representation with label encoding
le = LabelEncoder()
label = le.fit_transform(df['Spatial Distribution'])
label2 = le.fit_transform(df['Temporal Distribution'])
df.drop("Spatial Distribution", axis = 1, inplace = True)
df["Spatial Distribution"] = label
df.drop("Temporal Distribution", axis = 1, inplace = True)
df["Temporal Distribution"] = label2
df

#in these columns instead of ",", "." has been written for ratio numbers. for example 0,4 instead of 0.5
for x in range(len(df['T/R'])):
   df['T/R'][x] = df['T/R'][x].replace(",",".")
   
for x in range(len(df['Processor Utilization '])):
   df['Processor Utilization '][x] = df['Processor Utilization '][x].replace(",",".") 

for x in range(len(df['Channel Utilization'])):
   df['Channel Utilization'][x] = df['Channel Utilization'][x].replace(",",".")

#in these columns we just remove ',' sign to represent large numbers as integer
#for example instead of 1,111,234 we write as 11111234
for x in range(len(df['Channel Waiting Time'])):
   df['Channel Waiting Time'][x] = df['Channel Waiting Time'][x].replace(",","")

for x in range(len(df['Input Waiting Time'])):
   df['Input Waiting Time'][x] = df['Input Waiting Time'][x].replace(",","")

for x in range(len(df['Network Response Time'])):
   df['Network Response Time'][x] = df['Network Response Time'][x].replace(",","")

df.head()

#normalizing with minmax method
df2 = df.copy()
scaler = MinMaxScaler()
scaler.fit(df)
scaled = scaler.fit_transform(df)
df = pd.DataFrame(scaled, columns = df.columns)

for x in df.columns:
  df[x] = df[x].astype(float)
#df['Spatial Distribution'] = df2['Spatial Distribution']
df['Temporal Distribution'] = df2['Temporal Distribution']
df.head()

#we will do outlier analysis for only 4 columns using popular box plot method
print("Old Shape: ", df.shape)

for x in df.columns: 
  if x == 'Processor Utilization ' or x == 'Channel Waiting Time' or x == 'Input Waiting Time' or x =='Network Response Time':

    Q1 = np.percentile(df[x], 25,
                   interpolation = 'midpoint')
    Q3 = np.percentile(df[x], 75,
                   interpolation = 'midpoint')
    IQR = Q3 - Q1

    # Upper bound
    upper = np.where(df[x] >= (Q3+1.5*IQR))
    # Lower bound
    lower = np.where(df[x] <= (Q1-1.5*IQR))
 
    ''' Removing the Outliers '''
    df.drop(upper[0], inplace = True)
    df.drop(lower[0], inplace = True)
 
print("New Shape: ", df.shape)

X = df.drop(["Temporal Distribution"], axis = 1)
y = df["Temporal Distribution"]
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.25, random_state=42)

y_train.unique()

#OZELLIKLERIN ONEM DERECESINE GORE SIRALANMASINI XGB MODELINI KULLANARAK 
#GOSTEREBILIRIZ
from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X_train, y_train)
importances = pd.DataFrame(data={
    'Attribute': X_train.columns,
    'Importance': model.feature_importances_
})
importances = importances.sort_values(by='Importance', ascending=False)
plt.bar(x=importances['Attribute'], height=importances['Importance'], color='#087E8B')
plt.title('Feature importances obtained from coefficients', size=20)
plt.xticks(rotation='vertical')
plt.show()

#1st classification model type KNN with Cross Validation
parameter_range = [3,5,7,9,11,13,15]
for x in parameter_range:
  knn = KNeighborsClassifier(n_neighbors=x)
  cv_scores = cross_val_score(knn, X_train, y_train, cv=5)
  print('neighbor number - > ', x, 'score -> ', np.mean(cv_scores))

#we choose 15 as parameter
knn = KNeighborsClassifier(n_neighbors = 15)
knn.fit(X_train, y_train)
print("parameter -> ",15,"and the score -> ",knn.score(X_test, y_test))

predict_train = knn.predict(X_train)
print(confusion_matrix(y_train,predict_train))
print(classification_report(y_train,predict_train))

#Second model Naive Bayes
model2 = GaussianNB()
model2.fit(X_train, y_train)

print("the score -> ",model2.score(X_test, y_test))

predict_train = model2.predict(X_train)
print(confusion_matrix(y_train,predict_train))
print(classification_report(y_train,predict_train))

#Third Model is Decision Tree
#FIRST TIME WITHOUT A MAX DEPTH PARAMETER AND GINI SELECTOR
model3GINI = DecisionTreeClassifier()
cv_scores = cross_val_score(model3, X_train, y_train, cv=5)
print('score -> ', np.mean(cv_scores))

#with entropy
model3 = DecisionTreeClassifier(criterion = "entropy", max_depth = 3)
cv_scores = cross_val_score(model3, X_train, y_train, cv=5)
print('score -> ', np.mean(cv_scores))

model3GINI = model3GINI.fit(X_train, y_train)
print("the score with criterion Entropy -> ",model3GINI.score(X_test, y_test))

predict_train = model3GINI.predict(X_train)
print(confusion_matrix(y_train,predict_train))
print(classification_report(y_train,predict_train))

#NEURAL NETWORK
model4 = MLPClassifier(hidden_layer_sizes=(8,8), activation='relu', solver='adam', max_iter=500)
cv_scores = cross_val_score(model4, X_train, y_train, cv=5)

print('score -> ', np.mean(cv_scores))

model4 = model4.fit(X_train, y_train)
print("score -> ",model4.score(X_test, y_test))

predict_train = model4.predict(X_train)
print(confusion_matrix(y_train, predict_train))
print(classification_report(y_train, predict_train))

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0).fit_predict(X)

#print(kmeans[:1000])

y2 = y.to_numpy()
cnt = 0


#DEGERLENDIRME
for x in range(len(kmeans)):
  if kmeans[x] == y2[x]:
    cnt = cnt + 1


print('the score is -> ', cnt / len(kmeans))

#print(y2[:1000])

print(classification_report(y2, kmeans))

#print(kmeans.score(X_test, y_test))
# y_pred = kmeans.predict(X_test)
#print('score -> ', score)

#GAUSSIAN MIXTURE MODEL

gaussianmix = GaussianMixture(n_components = 2)
gaussian_result = gaussianmix.fit_predict(X)
gaussian_clusters = gaussian_result.unique


#DEGERLENDIRME
for x in range(len(kmeans)):
  if kmeans[x] == y2[x]:
    cnt = cnt + 1


print('the score is -> ', cnt / len(kmeans))

from matplotlib import pyplot
from numpy import where
birch_model = Birch(threshold = 0.03, n_clusters = 2)

birch_result = birch_model.fit_predict(X)

# get all of the unique clusters
birch_clusters = unique(birch_result)

# plot the BIRCH clusters
for birch_cluster in birch_clusters:
    # get data points that fall in this cluster
    index = where(birch_result == birch_clusters)
    # make the plot
    pyplot.scatter(training_data[index, 0], training_data[index, 1])

# show the BIRCH plot
pyplot.show()


#DEGERLENDIRME
for x in range(len(kmeans)):
  if kmeans[x] == y2[x]:
    cnt = cnt + 1


print('the score is -> ', cnt / len(kmeans))

#agglomerative hierarchy clustering algorithm
from sklearn.cluster import AgglomerativeClustering

agglomerative_model = AgglomerativeClustering(n_clusters=2)
agglomerative_result = agglomerative_model.fit_predict(X)
agglomerative_clusters = agglomerative_result.unique



#DEGERLENDIRME
for x in range(len(kmeans)):
  if kmeans[x] == y2[x]:
    cnt = cnt + 1


print('the score is -> ', cnt / len(kmeans))